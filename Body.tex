\section{Introduction}
Today the need for stable webapplications is vital for companys and startups. Established businesses need to be
available for their customers and users all the time. For Startups the availability of their services can be the difference
between failure and success. Webapplications and services needs to change and scale over time. Sometimes it is the need
to open up your service for more customers, like Runtastic, a small company from Linz/austria. They developed
a fitness application whose userbase was growing in a short periode of time. Somtimes the customerbase demands a 24/7
availability, for example the sales platform Amazon or the social network Facebook. When Facebook took an outage on September 28 2015,
this incident got a huge ammount of interest from local, national and international media (\url{http://www.bbc.com/news/world-us-canada-34383655}.

Downtime is costly. In the best case it only costs money, in the worst case
it can be the failure of your business or startup. All the mentioned services and events have one thing in common. To provide this kind of zero
downtime and continuous change in your webapplication you need a reliable and stable automatisation for your services. Development operations are
right now one of the major topics in webdevelopment. If used properly, DevOps can provide this kind of zero downtime and progress in an application
(\cite{humble2010continuous} \cite{duvall2007continuous}). To do so DevOps can use a variaty of techniques and methods to continuously
integrate, automatic test and deploy an applications (\cite{meyer2014continuous}).
Continuous Integration and development is based on the method of agile software development and extrem programming
(\cite{lindstrom2004extreme}). There is a reliable base of knowledge and literature for Continous Integration today
(\cite{schaefer2013continuous}), (\cite{fowler2006continuous}) (\cite{fowler2012continuous}).

However, some of the newer methods, for example MEAN stack or NodeJS are not yet fully covered.
The Question i will research in this thesis is, how can a modern webapplication automatized and deployed to continously integrate features and
changes. It is my goal to show how to establish a modern development operation cycle for webapplications using Node.js and the MEAN stack.
I will citeing the standard literature and methods for continuous integration and deployment and show how to build them in a safe and stable
way with Node.js and other Parts of the MEAN stack.
For this i will study the literatur that is available for continuous integration and autmatization to build a similar system with NodeJs.
To do so i will describe how to automatize builds and tests and how to use containerization to simplify the process. Based on the
implementation cycle of the neolexon webapplication, i will provide data to prove that this setup is capable of continous integration and deployment
ant that it runs in a stable and reliable.

\newpage

\section{Introduction to Agile Development and the Role of Development Operations in Modern Web Applications}
\label{section:Introduction to Agile Development and the Role of Development Operations in Modern Web Applications}

% start here
\subsection{Engineering Software}
Webdevelopment has its roots in general software engineering and uses most of the models, tools and methodologies that are
common in the field of software development. At the beginning of software development this new field starts with methodologies
and practices that a used, tested and accepted in other engineering subjects like civil engineering or architecture. In civil engineering
a project uses a design phase, which usually provides a detailed plan and, and a construction phase to build exactly after the specified
plan that where invented in the design phase (\cite{lindstrom2004extreme}). Usually, in civil engineering, the construction phase
needs the majority of time and money from the ressources for a given project. In his essay (\cite{fowler2001new}) Martin Fowler writes
that only 10 percent of the available ressources in civil engineering are used for designing and planing a project.
He concludes that using these engineering methodologys have brought software development into trouble, because they seperate design and
construction. To to so means to handle a project like a bridge or building.

A very detailed plan is made, and after a careful mathematical analysis it is descided to put this plan into practice. To construct,
another department or team will provide the actuall construction the bridge.
This seperation makes it possible to use people as ressources in construction, whereafter the design takes place in a more
intellecutal environment. The construction ressources in general dont need to understand the whole design and can concentrate on their
knowledge about constructing. In all this lies a predictability. The trouble for software development with seperation of design and
construction is that the whole process of programming is desiging in the first place. \cite{opac-b1105529} stats that only 15 percent of
a software project is code. This stands in contrast to the building example from civil engineering.

\cite{reeves1992software} therefore suggests that the code of a software project should be used as the design document and that the compilation of the
code should be seen as the main construction phase. For \cite{fowler2001new} the conclusion to Reeves suggestion is the following:
\begin{itemize}
  \item In software: construction is so cheap as to be free
  \item In software all the effort is design, and thus requires creative and talented people
  \item Creative processes are not easily planned, and so predictability may well be an impossible target.
  \item We should be very wary of the traditional engineering metaphor for building software. It's a different kind of activity and requires a different process
\end{itemize} \cite{fowler2001new}

\newpage

\subsection{The Agile Software Manifesto}
Traditional engineering metaphores and separation of design and construction are good for a predictable project where the outcome is clear in
detail to the customer. There is no such thing as mathematical analysis for software design plans and this means that there will be a very
limited predictability because of all the changes in software development (technical or customer based). To show this in practical detail consider the following
application. This application should provide information for speach therapists in a simple and understandable dashboard in which the user (therapist) gets
an overview. The descision was made that the dashboard should hold only the list of patients and wordlists (figure 1).

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/patientsandlists.png}
  \caption{first plan of therapiest dashboard}
\end{figure}

After the dashboard was created and the feedback of several therapists arrived the descision was made that the dashboard dont need to hold the lists
of words, because all lists where individual for the curent patient. This was a total different idea for the wordlists and not in the plan at the beginning.
after this descision the dashboard was not a dashboard anymore, but was left with only the patientlist alone. So the development team came up with another
idea of the dashboard (figure 2)

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/sidenav.png}
  \caption{final dashboard}
\end{figure}

\newpage

As you can see the two ideas differs in layout and idea. According to the initial plan the wordlists should be there. Now you have a sidenavigation
with patients and the statistics for each of them are not bound to the wordlist but are shown in the dashboard. This transformation would have been very
complicated to achieve with traditional engineering methaphore, because the klients only have had seen it at the end of the process. To rebuild a heavily
connected part of a webapplication after the application is finished is very hard to do. This changes toke place in only 2 weeks, from plan to final idea.
So another approache for software development is needed to take difficulties like the described into account.

To conquer this problem, 17 developers (Kent Beck, Mike Beedle, Arie van Bennekum, Alistair Cockburn, Ward Cunningham, Martin Fowler, James Grenning,
Jim Highsmith, Andrew Hunt, Ron Jeffries, Jon Kern, Brian Marick, Robert C. Martin, Steve Mellor, Ken Schwaber, Jeff Sutherland, Dave Thomas)
invented an idea of how to handle a softwareproject. On the 13. of february 2001 they created \textbf{\textit{a Manifesto for Agile Software Development}}.
The Manifesto itself is not a static rule, it is a guideline which can be followed. To make things practicable the Manifesto contains 12 principles about
agile developement.

\begin{itemize}
  \item Our highest priority is to satisfy the customer through early and continuous delivery of valuable software.
  \item Welcome changing requirements, even late in development. Agile processes harness change for the customer's competitive advantage.
  \item Deliver working software frequently, from a couple of weeks to a couple of months, with a preference to the shorter timescale.
  \item Business people and developers work together daily throughout the project.
  \item Build projects around motivated individuals. Give them the environment and support they need, and trust them to get the job done.
  \item The most efficient and effective method of conveying information to and within a development team is face-to-face conversation.
  \item Working software is the primary measure of progress.
  \item Agile processes promote sustainable development. The sponsors, developers and users should be able to maintain a constant pace indefinitely.
  \item Continuous attention to technical excellence and good design enhances agility.
  \item Simplicityâ€”the art of maximizing the amount of work not doneâ€”is essential.
  \item The best architectures, requirements and designs emerge from self-organizing teams.
  \item At regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior accordingly.
\end{itemize} \cite{fowler2001agile}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/manifesto.png}
  \caption{the agile manifesto}
\end{figure}

Based on the manifesto principles it is more realistic to build software. If you stay with the therapist dashboard example the agile approach makes more
sense, compared to a solid plan that starts at the beginning and did not change during the process, because most sofware change in the course of
development. Since many of the principles contain non technical ideas, like a cicle of team reflection, motivating team members or the art of simpicity,
there are solid ideas about how to treat the software and shipping process.

Right at the start of the manifesto the need for continuous delivery and integration is set as a high priority in development to satisfy the customer.
Satisfing customer needs is one good thing about this approach. Another reason to continuosly integrate software is about the change and the mantainability.
If an application or software changes fast in the development process it is a good idea to ensure that your software is working at every point,
even if the development team or single members change code or adding new features. It is possible to guarantee this in the beginning of a software project,
but when the software grows more complicated and complex it may be impossible to do.


\subsection{Continous integration}
Continouls integration is the process of continously create and ship code. CI can be described as merging new code with a set up project in short
development cyles constantly. To do this CI relies heavily on automated processes and tests. Automated testing in CI will help to catch issues before they
can to into the up and running service where they can cause errors and other troubles. The main technical idea behind CI is
one order philosphy. After developing new code the developer only needs to push the code to the repository. Through this action (pushing to a VCS) a chain
of events is triggered. The standard case is that the newly pushed scource code gets pulled and tested. If all tests pass and there are no problem the
application gets build and published. After the application gets published the development team gets notified to report that the testing and building was
successful. (\cite{stolberg2009enabling})

CI also helps to prevent integration problems where different people work on a project and all there code needs to integrate after a longer period of time.
With integration often (sometimes a few times per day) integrating code form different teams is more easy because the mass of code alone is very small in
comparison to an integration every few weeks.


\subsection{Continous integration and test driven development}
The main idea behind Test driven development is to ensure that the application or software stays functional at any level.
CI inherits the idea of using Test Driven Development. This means that you actually write test for the different parts (units) of your software.
In his book \textbf{\textit{Test-Driven Development by Example}} Kent Beck explains that writing automated tests before the actual code can be a good service
to an applications. As a developer starts to add features and introduce changes into an application that is already used, automated testing should be a standard.
Tests will make sure that problems with existing code or features will be visible after a developer changes existing parts in the software. That means that the
developer did not need to go through all parts of the application manually after some changes to ensure that everything is still working. To do so is timeconsuming
and inefficient, because it is still likley to oversee functionalities in the code that will be effected by different changes that are made afterwards. It also
leads to thought through programming decisions. Test-driven development focusses on how TDD leads analysis, design, and programming decissions
(\cite{janzen2005test}) which will be of help if the software application grows larger or more complex. Writing unit tests (a unit tests tests only a small
part of an application, for example if the email validation is working correctly) combined with continously integrate and delivery provides a
good setup for error free applications.

\subsection{Continous delivery and deployment}
Continous delivery is needed to deliver code to other services, teams and customers for review. After a successful review and if all tests are showing that the
application is error free, the application can be build and deployed. Figure 4 shows how a continuous delivery process runs in general.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/cd.png}
  \caption{continuous delivery cycle from https://blog.risingstack.com/continuous-deployment-of-node-js-applications/}
\end{figure}

\newpage

\section{Continuous Integration and Automatization}
\label{section:The Foundations of Continuous Integration and Automatization in Development Operations}

% start here
\subsection{The continous integration pipline}
Today building a web application contains different parts and techniques that needs to be combined. Besides the layout and look of the frontend applications
most services are using databases to store user data and let users authenticate to a service. There is a wide range of techniques in either, backend and
frontend, that can be used today to ship an application. For this thesis i will describe various options to create and deliver an modern web
applications with methods of continous integration, continuous delivery and continous deployment.

I will start to describe differences solutions to build a stable CI pipline. I also will explain what parts are essential for a CI pipline and what options
are available to choose from. Furthermore i will show an idea of a usable workflow in operations and CI. To build and automatize a CD prozess you have to build
a continouus deployment pipeline which contains all the assets, scripts and tools you need to build, test and deploy your application after you push it the a
version controll system. In figure 5 you see a very basic and general pipline without any specifics.


\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/pipeline.png}
  \caption{basic CD pipeline from https://blog.risingstack.com/nodejs-production-environment-for-startups/}
\end{figure}

To go into more detail about the CD pipeline the following stages are described in general. Theses Stages are the same for every software project using
continous delivery. They vary in some curcial point that i will later explain, but the basic is the same. To understand the CD pipeline the process needs
to be understand. The pipeline follows one stage after another and each stage triggers the start of the next one. The trigger prozess only starts if the
stage before passes tests or metrics. Humble and Jez describe these four stages in the book "continuous delivery: reliable software releases through build,
test, and deployment automation" (\cite{humble2010continuous})

\subsection{The commit stage}
The CD pipeline starts with the commit stage. Usually this stage is triggered whan a developer puhes new code to a repository at the version control system.
In this case a script should pull the new code and build the project. The purpuouse of the commit stage is to to single out builds with errors and therefore
are unfit to be released as a project to the production server (user/customer) and go give feedback about the build back to the developing team as soon as possible.
This stage consists of roughly five phases. At the beginning the code from the pulled repository gets compiled. After this the commit stage should hold the unit
tests for the project. Theses tests should run in the commit stage. It is possible to use more specific tests in this stage according to Jez and Humble
(\cite{humble2010continuous}) the commit stage already can hold tests for later stages, if it is clear that typical problems occure that can be singled out
in this first stage. Since a CD environment should be an evolving system the development team can start with unit testing only and later bring in the tests
that will help prevent known problems with their application right at the beginning of the pipeline.

After the testing code should be stored for use in later stages. The code also should be analysed for structural errors or difficulties and
artifacts should be stored as well to reuse it later on, for example to use them in the test databases if you use these kind of things. \cite{osherove2015art}
describes a unit test as "A unit test is a piece of code which calls another code to prove them true or false. The unit test is true if the asumed is true.
A unit is a method or function". This definition states that unit tests testing the smallest part of an application. They are based on the function level.
Because of this the development team needs to be careful not to rely on unit tests alone to show if the application is working. Even if unit tests are all
working and the feedback of them is positiv the overall functionality of an application can be broken. Because of the basic structure of unit tests the
overall context of what the application does or does not can get lost, and even if all functions are up and running, connected functionality can be damaged.
A rule for good unit test is that the should be isolated from different code to only controle and test one function (\cite{osherove2015art}).
In general the commit stage should use 5- 10 minutes to build and test the application, depending on the projects extend. (\cite{humble2010continuous}).

\subsection{Automated acceptance test stage}
A successful commit stage should trigger the automated acceptance test stage afterwards. In this stage the goal is to secure the applications behaviours and
deliver the contend and functionality that the team and a customer expects from the software. For example, if the webapplication should provide a booking system
for something it should be possible to book the content on the side. This is what a costumer would expect from the application and therefore this should be
tested. In this case the test should assume that someone can place an order and evaluate the different parts of this (input field, input evaluation, auto
complete, sending the order). In this area of testing not only functions are checked for functionality but the UI hat uses these functions. The accetance test
stage is more complex than the commit stage. In the CD pipeline after every successfull stage the effort and the amount of time that is used will grow.
After the functions are working you invest more to test the UI and other interconnected parts. Doing this helps to just invest more into things that are
already proven to be working on a smaller level.

\subsection{Manual test stage}
After the acceptance tests there should be a manual testing stage in wich testers or developers use exploratory methods to observe and test different behaviours
of the application. The reason to not all do the behavioural testing manually is to minimize the amount of error during people. Using acceptance test helps
to automatice repetetive and error prone processes and leave some specific cases to test by hand. This is useful because there can be some things that are hard
to test and should be left for human beeings.

\subsection{Release stage}
If all the tests are proving that the applications is release ready there should be a single action that releases the application to production and therefore
to the opne world. Usually it is the stage with the highest prority that is deployed the least often. In this case production. Humble describes that the pipeline
process helps get ready for the final stage. By proving that the code and scripts are working the development team gets more and more assured that the application
is working and at the end you deploy it to the production environment. With this approach you make sure that your code or your scripts wont be the failure if
the application causes problems. of course there is a residual risk if it is, but it is much easier than to investigate if the code or the environment is the
root of failure. Minimizing people error through automatisation is a major part of CD services, but there are some other aspects to taken into account. Humble and Jez writing
that you should be using a productin like environment for testing. This makes sense because the more a test environment differs from the actual environment
the less secure the developers can be that it will behave according to what they saw and tested on the testing environment.

\subsection{Delivery pipeline practises}
There are certain practises that can help to use the continuous delivery pipeline in a proper and useful way.
Here is a list of proven praticeses on this matter.

\begin{itemize}
  \item Only build your binaries once.
  \item Deploy in the same way to every environment you use.
  \item Deploy into a copy of your production environment.
  \item Each change should propagate through the pipeline instantly.
  \item If any part fails, stop the line and start working on a fix.
\end{itemize} \cite{humble2010continuous}

\subsubsection{Only build your binaries once}
Having a CD pipeline set up is a useful and timesaving environment. However, by using different environments, compilers, software version or even the same
compiler in a different version can change a lot of things. Each time you re-compile your commited code to a build there can be changes that are unforseen
and that results in little or even non visible changes in other software or version differences. Because of this it is the best practise to only compile your
code once and reuse this build without re-compiling it later on. The code that gets deployed to your production environment should be equal the same than
your build that is going throught the pipeline. Dont ever re-create your code (binaries are in this context a collection of executable code) after the first
build. The best way to do this is to store the build in a filesystem and reuse it for every other stage that gets triggered for the pipeline. This matters most
if you create a continuous delivery server on your own. The most modern CD services are taking care that your builds are stored and reused later on.

\subsubsection{Deploy in the same way to every environment you use}
For deployment you should use the very same process, wheter it is for production or to a developer machine. By doing so you are be able to run all your builds
and scripts multiple times in the same way as you did later on the productin server. As this is the most important build it is a good thing to know that if
you deploy it will be working, because you tested it in the very same way before on developement and test machines. For this you should use a basic script that
does the same things that are necessary for all your environments. If you have unique settings for some environments it is best practise to store these on the
environment itself. This prevents you from writing different scripts for all your environments. Jez write about storing these unique settings in the VCS and
trigger them by hostname. If you deploy to a special environment this specific hostname of that environment triggerst that unique script together or instead
of the basic script.

\subsubsection{Deploy into a copy of your production environment}
It is always a good idea to keep the environments you deploy for testing and staging as nearly to the later production environment. If this works on
your environments the devolopment team can be confident that the build will run on the live environment. You need to be careful, all this is a betting
system in a kind of way, with every stage you finalize you can get more confident and in the end the possibility that everything runs like intended is
very high. In software development like in all other parts of the real world everything is dependent to a general possibility, you cannot be 100 percent
assured but the possibility of failure can be minimized to a nearly non existing state.

\subsubsection{Each change should propagate through the pipeline instantly}
As shown in the description of the commit stage every push to the version control system should trigger these stage (commit stage). The stage starts
unit testing and if everything works it triggers the next stage. But what happens when, during the unit tests in the commit stage, other developers push
to the repository. It may not be vital for small developer teams but even there different developers can push in short time to the repository. This results
in a build that gets tested and deployed, but is actually behind the real build that gots pushed by the team. A solution is an intelligent scheduling
system of your CD pipeline. Every time a stage is finalized the CD server search for changes in the repository before going to the next stage. In the
example i created the CD server starts the commit stage after the repository on the version controll system changes. After successfully ending the unit
tests the server checks the repository. Since a new commit was been pushed by another developer the CD server did not trigger the acceptance test stage.
Instead the starts building new and does the unit tests with the now actual build. If other developerst commit code the same rule applies to the end of the
commit stage. In this case the build and deploy process slows down, because the commit stage took longer because of restarting several times with a new
commit. But compared to having a deployed build that is behind the real code a longer time can be neglected. There is only so much code that can be shipped
the same time so eventually the pipeline will come to an end. If the development team is really big you can schedule commit times, but this will theoretically
stand against the theory of CD. If you look at it there will be an even longer process with commit times, because if every commit goes through the whole
pipeline but is not the last commit, you ending up with more time until you get an actual deployed build.

\subsection{Starting a continuous delivery pipeline}
Starting a CD pipeline can be done in several ways. Before this thesis go to the actual tools that can be used to achiev a good workflow and pipeline
it will describe a basic starting concept. This can be used with any tool a team or developer choose to use for creating a CD service for their software
or application. This section assumes that there is a new project which starts at the iteration zero (this means there is no or only few code and tests, or
only prototype material that will be used partly later on). In this case a CD pipeline can be started at the bare minimum and than can grow with the project
or application. The bare minimum in this case is a commit stage that starts after a commit to the repository, a acceptance stage that starts right after the
commit stage and a deployment stage which deploys the application in a prodctin like environment. To start with non of the stages should do anything (
because there is no test or code right now). The only thing that should happen is that a push triggers the pipeline cascade so that all stages a running
and in the end there is a deployment. In doing so you have a placeholder CD pipeline that runs every time you change the repository. To populate the pipeline
and to test the system itself use a small part of code that can be brought in with the single purpouse to test the CD pipeline. As an example you can start a
simple hello world example as a console.log or html tag and a unit test that asserts to true if the console.log or html is there. With this minimalistic code
you can test your CD system by commiting this to the repository. The unit test should assert to true and after the commit stage the html should be deployed
to run an acceptance test (an acceptance test can only be running if a website is deployed to a staging server!). The acceptance test stage should than run
and verifys that the console.log or html is visible on the website. This first steps will provide a practical understanding of how the pipline works and
if it runs according to what you need to do with more complicated things later. In the next section i will show and describe different tools and CD services
that can be used to build an effective and stable continuous delivery chain.
% 4704 words!

\newpage


\section{Development and Deployment with MEAN}
\label{section:Development and Deployment with MEAN}

\subsection{Introduction}
Like mentioned in the last chapter, after the basic functions and implementations of the CD pipline are covered, this section will name and explain different tools
to set up a CD Pipeline. This paper has the goal to cover the implementation of a CD pipline for todays web applications. Since there are many tools out there
this thesis will not cover all of the known tools available. In fact the descriped software tools are suggestions to build a stable pipeline for a CD process.
To better understand and learn how to build the pipeline with the mentioned sofware, i will provide the building of a webapplication based on the
MEAN stack. MEAN stands for MongoDB, ExpressJS, AngularJS, and NodeJS. There are different papers and books about CD with different software systemst. So far
now on covers the the building with the MEAN stack.
% plz research and prove this claim with literature!!!
The reason to choose the MEAN stack for the CD example application is that MEAN uses modernt frameworks that are commonly practised by webdevelopers. Technology
will evolve and eventually singe page applications build with the MEAN stacke will be out of date. But so far this approach holds many standards in modern web
development.

\subsection{Describing the MEAN stack}
Before starting the tools and practises for building the pipeline, a short description of the MEAN stack will help to understand how the different parts of the
stack are working together and how they will be setup. After this short introduction, a practical example of how to use the whole MEAN stack will be described.
This Example will be a step by step MEAN stack setup which includes code and references to do so.

\begin{itemize}
  \item MongoDB
  \item ExpressJS
  \item AngularJS
  \item NodeJS
\end{itemize}

\subsubsection{MongoDB}
MongoDB is a non-relational database which is based on a JSON (JavaScript Object Notation) like document structure. In Detail MongoDB documents are using the BSON structur
which is very similar to JSON. A document consists of field-value pairs. As an example i will show the document structure of a patient inside the mentioned neolexon application.

\begin{verbatim}
  {
    _id: "56e515ee8ad50f08e1886f13",
    picture: "http://placehold.it/32x32",
    age: 40,
    firstname: "Vroni",
    lastname: "Musterhuber",
    gender: "female",
    about: "Some information"
  }
\end{verbatim}

The code example shows a MongoDB document with different field-value pairs. In this case the fields are id, picture, age, firstname, lastname, gender and
about. The values the assigned values per field. For example, the value for the field firstname is Vroni. The Mongo database itself is a \textit{Collection}
of \textit{Documents}. The above code shows a \textit{Documents} and many \textit{Documents} together are a \textit{Collection}.

MongoDB uses dot notation to access fields inside a \textit{Documents}. If a developer needs to access the firstname field in the above document, he or she
needs to concatinate the \textit{Documents} and the field like this <document>.<field>. Let us assume there are many patients inside our collection and we
want to query for all patients that older than 30. In this case the query would contain the following code:

\begin{verbatim}
db.patients.find({age: {$gt: 30}}).sort({age: 1})
\end{verbatim}

patients are representing the \textit{Collection} (all patients in the database). Find, decribes what should be done. In this
case we would like to single out specific patients from our database. The criteria is to find all patients with an age greater than 30. The last part is
a modifier which works like a filter. It allows to sort the findings from this querie after the age (from youngest to oldest). There are different
chains for a MongoDB query. They are all explained in detail on the MongoDB website under \url{https://docs.mongodb.org/manual/core/crud-introduction/}.

The advantge of using MongoDB in our app is that you dont need another programming language like \textit{SQL} for relational databases.
For Mongo you can use the programming language you are working with in you application. In this case JavaScript. Since the MongoDB document structure is
similar to JSON it allows to easily access and use the data inside your database. \cite{chodorow2013mongodb}

\subsubsection{ExpressJS}
The next part of the MEAN stack is the ExpressJS framework. The ExpressJS website describes it as a fast, unopinionated and minimalistic web framework
for NodeJS \url{http://expressjs.com}. In fact you do not need ExpressJS for your sofware stack. You can do all the connections and backend operations with
NodeJS alone. The advantage of ExpressJS is that it helps to connect all the different parts from your backend. That is why the MEAN stack uses the framwork.
The MEAN application for this thesis will show the advantages an application gets from using ExpressJS later in the practical part and code examples.
ExpressJS glues all needed parts of the application together, so the developer dont need to use and configure all backend and connectin parts manually
\cite{brown2014web}. To do this ExpressJS requires init and configuration files from the application. As long as you put the needed files into the right
directory in the webapplication, ExpressJS will find them and makes use of them. ExpressJS also handles sessions for the login and doing the error handling
on the backend side.

\subsubsection{AngularJS}
% more research about what is it and  why should be used for MEAN and what are the specifics someone needs to know
If ExpressJS is the framework for the backend, in this case NodeJS, AngularJS is the javascript frontend framework. As said for ExpressJS you do not need
AngularJS to develop the application. But Angular will make it more comfortable to write frontend functionality. Angular is an MV* framework. This stands
for \textbf{M}odel - \textbf{V}iew - *. The * acutally is a placeholder and means you can use Angular in different ways. Be it MVC - Model View Controller
or als MVVM - Model View View Model. This depends on the choices of the development team and what application you will build. The example application i
use for this thesis is a single page MVC application. It will have a Model which handles the logic and a Controller, which refers to different Views of
the application. A MEAN stack application will handle differen models, controllers and view so seperate concerns between different functionalities.

\subsubsection{NodeJS}
In \textit{Node up and running} (\cite{hughes2012node}) NodeJS is described as a non blocking, scalable javascript framework. The major advantage is that
if the development team uses javascript in the frontend there is no need to transfer functionalities into php or other backend programming languages.
This makes it easy to use the same language for back- and frontend, javascript. Besides the scalability of a non blocking architecture and the performance
of the V8 javascript runtime enginge, NodeJS posesses a very large packaging ecosystem (node package manager) which makes it comfortable to work with.
% more research about what is it and  why should be used for MEAN and what are the specifics someone needs to know

\newpage

\subsection{Continuous delivery pipeline: tools and practices}
After this brief introduction of the different parts of the MEAN stack, this thesis describe the tool that will be used for the MEAN stack and a
continuous delivery pipeline. However this is not a complete list. Appart from the MEAN software stack the neolexon
example application will be using the following tools and software:

\begin{itemize}
  \item Yeoman
  \item StriderCD
  \item Karma
  \item Angular Mocks
  \item Protractor
  \item Mocha
  \item Chai
  \item Sinon
  \item Browserify
  \item Partialify
  \item Lodash
  \item Istanbul
  % are there more? Plz make a list and work through them!
\end{itemize}
% description of all the tools

% here follows the description of the meanjs generator, structure and setup
\section{Describing CD Pipeline (WIP)}
\label{section:Describing CD Pipeline (WIP)}
To begin the setup process the beginning of this chapter shows the versions and requirements the neolexon project uses for the CD pipeline and MEAN stack.
I will split the configuration into server and client side and following by this there will be a step by step guide on how to setup and what problems and
difficulties can be occure.

\subsection{Software And Versions}
The neolexon project is using \textbf{Ubuntu 14.04} as the server operation system. The following versions are installed on the server:
\begin{itemize}
  \item node: v4.0
  \item npm: v2.0
  \item git: v2.0
  \item mongoDB: v3.2.3
  \item node-gyp: v3.3.0
  \item StriderCD v1.7.7
\end{itemize}

Strider v1.7.7 has some requirements that are shown in the actual documentations. Our setup takes care of the required versions for SriderCD v1.7.7.
If you are not familiar with setting up versions and software on an Ubuntu server you can just follow the installation instructions in the different
software documentation. For using the exact versions listet above it is useful to use the node version manager (nvm) so you can change to node v4.0 if
you installed the latest version.

This Thesis assumes that we setup up Strider to deploy on DigitalOcean. DigitalOcean offers a viritual private server (VPS) for only 5 euro per month. This
includes 1 Gigabyte of transfer and 20 Gigabyte diskspace. If you need a higher transferrate or more diskspace int he future it is easy to change the
subscription. You also can choose a datacenter which stores your data. In the neolexon case the data needs to stay in the european union for privacy
and data security reasons. If this is an option to consider for your application, this can be a useful feature to choose. As most services today do, you
can use your DigitalOcean VPS together with an SSH key to log in.

In order to use the DigitalOcean service with your Ubuntu server you need to setup an account at \url{http://digitalocean.com/registrations/new}. After
registration and confirmation via email the next step is to create a droplet for your Ubuntu 14.04 server. A droplet is the DigitalOcean name for a  their VPS.
In our case we use the Ubuntu 14.04 x64 droplet. The setup in general is a straight forward process. In case of difficulties or problems DigitalOcean has
created a tutorial for this. It can be found under \url{https://www.digitalocean.com/community/tutorials/how-to-create-your-first-digitalocean-droplet-virtual-server}.
We will be using the 64-bit structure for the new VPS even the VPS size ustes 512MB of RAM. With RAM of 1 Gigabyte or less the performance benefits of a 64-bit architecture
will be diluted because the less memory. However, we found this will be the system of choice because of a future upscale of the application itself. If you plan
to scale your application this can be an option to reconsider.

To use the new created VPS you can log into it by using an SSH key DigitalOcean will send to you after creating the droplet/VPS. The SSH login authentication has a security
advantage over the common login practise. To install the needed software or version changes and updates use the \textit{ssh root@<droplet/vps ip>} with
the terminal/console and authenticate the host by setting up a fingerprint for the connection. After you the login process it is vital to change the password for
security reasons. The best way is to use the \textit{passwd} command and choose a save password.

\subsection{Installing The MEANJS Application}
To start the Application itself the neolexon project uses the MEANJS yeoman generator. If you are not familiar with the yeoman service this will summarize what what
it does and how to setup the MEAN application. Yeoman is a scaffolding tool that can be installed and let you chooes, install and setup applications. The
application setup process is done through generators. Generators installing the necessary software and configure it. They also using package managers like bower
and npm to install dependencies for the used application. There are many generators provided by different organistions or the yeoman team itself. In case of the
yeaoman MEANJS generator the MEANJS team has build the generator. The provided generator is the main reason to use MEANJS over MEAN.io. In general the two stacks
are similar in use and what they do. However setting up a MEAN application through yeoman is a fast an straightforward process that helps to bind all the different
parts. The MEANJS Website provides a good documentation of how to use the yeoman generator (\url{http://meanjs.org/generator.html}).

The basic terminal command is to install yeoman with npm by using

\begin{lstlisting}
 npm install -g yo
\end{lstlisting}

Yeoman is installed and the next step is to install the needed generator. In the neolexon case we use the MEANSJ generator. The proper command for this is

\begin{lstlisting}
 npm install -g generator-meanjs
\end{lstlisting}

The last step is to use this generator to install the MEANJS app. The following command lets yeoman build the above installed generator

\begin{lstlisting}
   yo meanjs
\end{lstlisting}

Yeoman will taking care of installing the application after some minor questions about the name, origin and description of the application.
It will setup the structure and with it the \textit{package.json} and \textit{node_modules}.

\begin{figure}[h!]
  \centering
      \includegraphics[width=0.2\textwidth]{images/yomean.png}
  \caption{Exampelimage}
\end{figure}

As soon as the application is installed and build you can start it with

\begin{lstlisting}
   grunt
\end{lstlisting}

Which starts the grunt task runner and provides the application unter \textit{localhost:3000} in any browser of your choice.




\section{Strider Configuration}
\label{section:Strider Configuration}


\section{Test Driven Development with MEAN}
\label{section:Test Driven Development with MEAN}

% start here

\newpage

\section{Setting Up The Mean Applicaton}
\label{section:Containerization and Deployment with Docker}

% start here

\newpage

\section{Building Continuous Integrations and Deployment with NodeJS}
\label{section:Building Continuous Integrations and Deployment with NodeJS}

% start here

\newpage

\section{Containerization and Deployment with Docker}
\label{section:Containerization and Deployment with Docker}

% start here

\newpage

\section{Practical Example Code}
\label{section:Practical Example Code}

% start here

\newpage





% \begin{figure}[h!]
%   \centering
%       \includegraphics[width=0.4\textwidth]{images/Perlin-Coherent.png}
%   \caption{Just some example figure}
% \end{figure}



% \subparagraph{subparagraph}
% \footcite{meyer2014continuous}
%
% \begin{itemize}
%   \item Itemlist 1
%   \item Itemlist 2
% \end{itemize} \cite{cranorplatform}
%
% \section{Next Section}
% \label{section:Label}
%
% \textit{Texit Option}
%
% \begin{figure}[h!]
%   \centering
%       \includegraphics[width=0.2\textwidth]{images/Julia-Fractal.png}
%   \caption{Exampelimage}
% \end{figure}
%
% \subparagraph{Unforgeability}
% \label{subp:subparagraph_name}
%
% Graphic by \url{http://en.wikipedia.org/wiki/Pretty_Good_Privacy#/media/File:PGP_diagram.svg
