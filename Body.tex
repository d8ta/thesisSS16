\section{Introduction}
Today the need for stable web applications is vital for companies and startups. Established businesses need to be
available for their customers and users all the time. For Startups the availability of their services can be the difference
between failure and success. Web applications and services needs to change and scale over time. Sometimes a service gets customers or is
growing fast. Runtastic, a small company from Linz/Austria which developed a fitness application where having a tremendous growth of
their user base, in a short period of time. Most of the time users demands a 24/7 availability of the service they use a good example
is the sales platform Amazon or social networks like Facebook. When Facebook experienced an outage on September 28 2015, this incident
got a huge amount of interest from local, national and international media \footnote{http://www.bbc.com/news/world-us-canada-34383655 visited on 13. April 2016}
and from their user base.

Downtime is costly. In the best case it only costs money, in the worst case
it can be the failure of your business or startup. All the mentioned services and events have one thing in common. To provide this kind of zero
downtime and continuous change in your web application you need a reliable and stable automatization for your services. Development operations are
right now one of the major topics in web development. If used properly, DevOps can provide this kind of zero downtime and progress in an application
(\cite{humble2010continuous} \cite{duvall2007continuous}). To do so DevOps can use a variety of techniques and methods to continuously
integrate, test and deploy applications (\cite{meyer2014continuous}).
Continuous Integration and development is based on the method of agile software development and extreme programming
(\cite{lindstrom2004extreme}). There is already a reliable base of knowledge and literature for Continuous Integration in todays web developer community
(\cite{schaefer2013continuous}), (\cite{fowler2006continuous}) (\cite{fowler2012continuous}).

Some of the newer methods, for example MEAN stack or NodeJS applications are insufficient investigated and described.
The Question this thesis will research is: What is a best practice approach for modern web application,
to provide automatization and continuously integration features. It is the goal of this thesis to show how to establish a modern development operation
cycle for web applications using Node.JS and the MEAN Stack.
This thesis will citing the standard literature and methods for continuous integration and deployment. After this this thesis will
show how to build a MEAN Stack application in a safe and stable way.
For this the thesis will research the literature available for continuous integration and automatization, to build a similar system with the MEAN Stack.
To do so the thesis will describe how to automatize builds and tests. The description will be based on the implementation cycle of the neolexon web application.

\newpage

\section{Introduction to Agile Development and Development Operations}
\label{section:Introduction to Agile Development and Development Operations}

\subsection{Engineering Software}
Web development has its roots in general software engineering and uses most of the models, tools and methodologies that are
common in the field of software development. At the beginning of software development, this new field starts with methodologies
and practices that are used, tested and accepted in other engineering subjects, like civil engineering or architecture. In civil engineering
a project uses a design phase, which usually provides a detailed plan and which culminates into a construction phase. There the project
will be build exactly after the specified plan that was invented in the design phase (\cite{lindstrom2004extreme}).
In civil engineering the construction phase uses the majority of the time, money and the resources given for the project.
In his essay (\cite{fowler2001new}) Martin Fowler writes that only 10 percent of the available resources in civil engineering are used for
designing and planing a project. The other 90 percent goes into construction. Fowler concludes that using these engineering methodologies have
brought software development into trouble, because separating design and construction, common in other fields of engineering, like building
a bridge does not fit the needs of software engineering.

In civil engineering a very detailed plan is made, and after a careful mathematical analysis it is decided to put this plan into practice. For constructing,
for example a bridge, the construction department will in detail and exactly after the made plan.
This separation makes it possible to use people as resources in construction, whereafter the design takes place in a more
intellectual environment. The construction resources in general don't need to understand the whole design and therefore can concentrate on
concentrate about the construction. The trouble for software development with separation of design and construction is, that the whole process of programming
, according to fowler, is designing in the first place. \cite{opac-b1105529} stats that only 15 percent of a software project is code.
This stands in contrast to the building example from civil engineering in which the most percentage of time and resources going into construction.

Becaues of this reversal \cite{reeves1992software} suggests that the code of a software project should be used as the design document and that the compilation of the
code should be seen as the main construction phase. For \cite{fowler2001new} the conclusion to Reeves suggestion is the following:
\begin{itemize}
  \item In software: construction is so cheap as to be free
  \item In software all the effort is design, and thus requires creative and talented people
  \item Creative processes are not easily planned, and so predictability may well be an impossible target.
  \item We should be very wary of the traditional engineering metaphor for building software. It's a different kind of activity and requires a different process
\end{itemize}

\newpage

\subsection{The Agile Software Manifesto}
Traditional engineering metaphors and separation of design and construction are good for a predictable project where the outcome is clear in
detail to the customer. In software engineering and web development there is proven method to mathematically analysis a software design. This means that there is
a limited predictability because of changes that appear in software development (technical or customer based) most of the time. To show this in practical detail
consider the following neolexon example application. The web application should provide information for speech therapists in a simple and understandable dashboard,
in which the user (therapist) gets an overview about what is going on. The decision was made that the dashboard should hold only the list of patients and wordlists (figure 1).

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/patientsandlists.png}
  \caption{first plan of therapist dashboard}
\end{figure}

After the dashboard was created and the feedback of several therapists arrived, the decision was made that the dashboard did not need to hold the lists of words, because all lists where individual to the patients.
after this decision, was left with only the patient list. In case of the application this was not the main goal for the invented dashboard. To conquer this the development team came up with another
idea of the dashboard (figure 2)

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/sidenav.png}
  \caption{final dashboard}
\end{figure}

\newpage

As seen in the above figures the two ideas are very different in layout, design and idea. According to the initial plan the wordlists should be there.
Now you have a side navigation with patients and the statistics for each of them. This transformation would have been very
complicated to achieve with traditional engineering metaphor, because the clients only would have seen it at the end of the process. To rebuild vital
parts of a web application is a complicated process. The changes occurred in a timeframe of 2 weeks, from plan to final idea.
This example shows that traditional engineering strategies are not working well in terms of web development and software.

To conquer the problem, 17 developers (Kent Beck, Mike Beedle, Arie van Bennekum, Alistair Cockburn, Ward Cunningham, Martin Fowler, James Grenning,
Jim Highsmith, Andrew Hunt, Ron Jeffries, Jon Kern, Brian Marick, Robert C. Martin, Steve Mellor, Ken Schwaber, Jeff Sutherland, Dave Thomas)
invented an idea of how to handle software project. On the 13. of February 2001 they created \textbf{\textit{a Manifesto for Agile Software Development}}.
The Manifesto itself is not a static rule, it is a guideline which can be followed. To make things practicable the Manifesto contains 12 principles about
agile development.

\begin{itemize}
  \item Our highest priority is to satisfy the customer through early and continuous delivery of valuable software.
  \item Welcome changing requirements, even late in development. Agile processes harness change for the customer's competitive advantage.
  \item Deliver working software frequently, from a couple of weeks to a couple of months, with a preference to the shorter timescale.
  \item Business people and developers work together daily throughout the project.
  \item Build projects around motivated individuals. Give them the environment and support they need, and trust them to get the job done.
  \item The most efficient and effective method of conveying information to and within a development team is face-to-face conversation.
  \item Working software is the primary measure of progress.
  \item Agile processes promote sustainable development. The sponsors, developers and users should be able to maintain a constant pace indefinitely.
  \item Continuous attention to technical excellence and good design enhances agility.
  \item Simplicity—the art of maximizing the amount of work not done—is essential.
  \item The best architectures, requirements and designs emerge from self-organizing teams.
  \item At regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior accordingly.
\end{itemize} \cite{fowler2001agile}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/manifesto.png}
  \caption{the agile manifesto}
\end{figure}

Based on the manifesto principles it is more realistic to build software, because the principles take the different approach of design and construction into
account. The above described therapist dashboard example would be making more sense, with the agile approach. Compared to a solid plan that starts at the beginning
without change during the process, agile development takes into account that most software is changing over the course of the
development. Since many of the principles contain non technical ideas, like a cycle of team reflection, motivating team members or the art of simplicity,
there are solid ideas about how to treat the software and shipping process.

At the start of the manifesto the need for continuous delivery and integration is set as a high priority in development to satisfy the customer needs.
But satisfying customer needs is only one part about this approach. Another reason to continuously integrate software is about the change and the maintainability.
If an application or software changes fast and often in the development process, it is a good idea to ensure that your software is working at every point.
It should be possible to assure working code from the beginning of a software project. Software will getting more complicated and complex over the course of the
project. To have working code throughout the project at every stage is beneficial.


\subsection{Continuous integration}
Continuous integration is the process of continuously create and ship code. CI can be described as merging new code with a set up project in short
development cycles constantly. To do this, CI relies on automated processes and tests. Automated testing in CI will help to catch issues before they
are deployed into live web application. The main technical idea behind CI is
the one order philosophy. After developing new code the developer only needs to push the code to the repository. Through this action (pushing to a VCS) a chain
of events is triggered. The standard case is that the newly pushed source code will get pulled and build by the CI service. After this the CI runs the implemented
test scripts. If all tests are passing can be deployed. Usually the development team gets notified about the successful or unsuccessful building process, so
it can respond by deploying the application or fixing the issues that the test are revealing (\cite{stolberg2009enabling})

CI also helps to prevent integration problems with different people working on one project. All the different code needs to integrate into the main project.
With integration often (sometimes a few times per day) integrating code from different team members or departments, become more easily,
because the amount of code alone is smaller, compared to longer integration cycles.


\subsection{Continuous integration and test driven development}
The main idea behind Test driven development is to ensure that the application or software stays functional at any level.
CI inherits the idea of using Test Driven Development. The development team writes the tests for different parts (units, for example different functions)
before the actual part gets developed and integrated.
In his book \textbf{\textit{Test-Driven Development by Example}} Kent Beck explains that writing automated tests before the actual code can be a good service
to an applications. As developers are starting to add features and introduce changes into an application that is already in use, automated testing should be a standard.
Tests will make sure that problems with existing code or features will be visible after existing software parts getting changed. That means that the
developer team did not need to go through all parts of the application manually, after changes are made to ensure a working web application. Manual testing is
time consuming and inefficient, because it is still likely to oversee functionalities in the code that will be effected by different changes that are made afterwards.
It also leads to thinking about the programming decisions that are made. Test-driven development is focused on analysis, design, and programming decisions are made.
(\cite{janzen2005test}) This will be of help, if the web application grows larger or more complex. Writing unit tests (a unit tests tests only a small
part of an application, for example if the email validation is working correctly) combined with continuously integrate and delivery provides a
good setup for error free applications and software that is working from start.

\subsection{Continuous delivery and deployment}
Continuous delivery is needed to deliver code to other services, teams and customers for review. After a successful review and if the tests are showing that the
application is working, the application can be build and deployed. Figure 4 shows how a continuous delivery process runs in general.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/cd.png}
  \caption{continuous delivery cycle from}
\end{figure}
\footnote{https://blog.risingstack.com/continuous-deployment-of-node-js-applications/ visited on 9. April 2016}

\newpage

\section{Continuous Integration and Automatization}
\label{section:Continuous Integration and Automatization}

\subsection{The continuous integration pipeline}
Today building a web application contains different parts and techniques that needs to be combined. Besides the layout and look of the front end application,
most services are using databases to store user data and let users authenticate to a service. There is a wide range of techniques in either, backend and
front end, that can be used today to ship an application. For this thesis i will describe how to create and deliver an modern web
applications with methods of continuous integration, continuous delivery and continuous deployment.

The start is a description of differences solutions to build a stable CI pipeline. I also will explain what parts are essential for a CI pipeline and what options
are available to choose from. Furthermore this thesis will show a best practice approach for a usable workflow in operations and CI. To build and automatize a CD process,
a continuous deployment pipeline is needed. This pipeline contains all the assets, scripts and tools to build, test and deploy the application after new code gets
push version controls system. In figure 5 a basic pipeline can be seen.


\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/pipeline.png}
  \caption{basic CD pipeline}
\end{figure}
\footnote{https://blog.risingstack.com/nodejs-production-environment-for-startups/ visited on 13. April 2016}

To describe the CI pipeline with more detail, the following stages are described. Theses Stages are the same for every software project using
continuous delivery. They vary in different parts,  that will be explained. The basics however, are the same. The pipeline follows one stage after another.
Each stage triggers the start of the next one. The trigger process only starts if the stage before, passes all tests or metrics.
Humble and Jez describe these four stages in there book "continuous delivery: reliable software releases through build,
test, and deployment automation" (\cite{humble2010continuous})

\subsection{The commit stage}
The CD pipeline starts with the commit stage. This stage is triggered when a developer pushes new code to a repository.
In this case a script pulls the new code and builds the project. The purpose of the commit stage is to single out builds with errors that
are unfit to be released to the production server.  Another purpose is to give feedback about the build to the development team as quickly as possible.
The commit stage consists of five phases. At the beginning the code from the pulled repository will be compiled. The commit stage holds the unit
tests for the project. The units tests are running inside the commit stage. It is possible to use more specific tests in this stage according to Jez and Humble
(\cite{humble2010continuous}). The commit stage already can hold tests for later stages, if it is clear that typical problems occurs that can be singled out
in this first stage. Since a CD environment is an evolving system the development team can start with unit testing only and bring in tests
that will help prevent known problems with their application right at the beginning of the pipeline.

The build should be stored for use in later stages, so you do not have to build it again and again for different purposes.
The code also should be analyses for structural errors or difficulties. Artifacts should be stored as well to reuse it later on, for example to use them in
the test databases if used. \cite{osherove2015art}
describes a unit test as "A unit test is a piece of code which calls another code to prove them true or false. The unit test is true if the assumed is true.
A unit is a method or function". This definition states that unit tests testing the smallest part of an application. They are based on the function level.
Because of this the development team needs to be careful not to rely on unit tests alone to show if the application is working. Even if unit tests are all
working and the feedback is positive, the overall functionality of an application can be broken. Because of the basic structure of unit tests the
overall context of what the application does or does not can get lost, and even if all functions are up and running, connected functionality can be damaged.
A rule for good unit test is that they should be isolated from different code to only controls and test one function (\cite{osherove2015art}).
The commit stage should run 5- 10 minutes to build and test the application, depending on the projects extend. (\cite{humble2010continuous}).

\subsection{Automated acceptance test stage}
A successful commit stage will trigger the automated acceptance test stage afterwards. The goal of this stage is to secure the applications behaviors and
deliver the contend and functionality, that the team or a customer would be expect from the application. For example, if the web application should provide a
booking system, it should be possible to book the content on the side. This is what a costumer would expect from the application and therefore this should be
tested. In this case the test should assume that someone can place an order and evaluate the different parts of this (input field, input evaluation, auto
complete, sending the order). This area of testing does not only test functions about the right behavior, but the whole UI that uses these functions.
The acceptance test stage is more complex than the commit stage. In a CD pipeline every successful stage adds effort and the amount of time that is used to test.
If the functions are working the service will invest more time and resources test the UI and other interconnected parts. Doing this helps to just invest more
into things that are already proven to be working on a smaller level.

\subsection{Manual test stage}
The following stage is manual testing. In this stage testers or developers use exploratory methods to observe and test different behaviors
of the application. The reason to do not test all the behavioral parts in the application manually, is to minimize the amount of error by people.
Using acceptance test helps to automatic repetitive and error prone processes and leave some specific cases to test by the team manually.
This is useful because there can be some things that are hard to test. They are best to be left for human beings.

\subsection{Release stage}
If all the tests are proving that the applications is a release ready candidate, there should be a single action that releases the application to production and
live servers. Usually it is the stage with the highest priority that is deployed the least often. In this case the production. Humble describes that the pipeline
process helps get ready for the final stage. By proving that the code and scripts are working the development team gets more and more assured that the application
is working and at the end you deploy it to the production environment. With this approach you make sure that your code or your scripts wont be the failure if
the application causes problems. Of course there is a residual risk that it is, but it is much easier to investigate if the code or the environment is the
root of failure. Minimizing people error through automatization is a major part of CD services, but there are some other aspects to taken into account. Humble and Jez writing
that a production like environment should be used for testing. This makes sense because the more a test environment differs from the actual environment
the less secure the development team can be, that it will behave according to what they saw and tested on the testing environment.

\subsection{Delivery pipeline practices}
To set up a CI pipeline in the above mentioned way, the following practices are recommended and helpful in the process.

\begin{itemize}
  \item Only build your binaries once.
  \item Deploy in the same way to every environment you use.
  \item Deploy into a copy of your production environment.
  \item Each change should propagate through the pipeline instantly.
  \item If any part fails, stop the line and start working on a fix.
\end{itemize} \cite{humble2010continuous}

\subsubsection{Only build your binaries once}
Having a CD pipeline set up is a useful and timesaving environment. However, by using different environments, compilers, software version or even the same
compiler in a different version, can be challenging. Each time the project re-compile the committed code into a new build unforeseen changes can occur.
The results of different build are little or even non visible changes that can cause problems. Slightly different software version or alterations can change
vital parts of the applications behave. Because of this it is the best practice to only compile your
code once and reuse this build without re-compiling it in later stages. The code that gets deployed to your production environment should be equal to the same
code that was going thought the pipeline. Re-create a build again (binaries are in this context a collection of executable code) after the first
build is considered a bad practice. The best way to do this is to store the build in a filesystem and reuse it for every other stage that gets triggered in the
pipeline process. This matters most if the continuous delivery server is custom build. Most of the modern CD services are taking care of that processes
themselves. (\cite{humble2010continuous})

\subsubsection{Deploy in the same way to every environment you use}
The employment should use the same process, whether it is for production or to a developer machine. By doing so all builds and scripts are able to
running in multiple environments, in the same way as later on the production server. The production environment this is the most important environment.
It is a good thing to be sure that the deployed web application will be up and running. If it is tested in the same way on development and test environments
the confidence to have a working web applications grows. For this a basic script that does the same is necessary for all the environments. If there are
unique settings for some of the environments it is a best practice advice to store the specific settings inside the environment itself. This prevents
writing different scripts for all the environments. Jez writes about storing these unique settings in the VCS to trigger the usage by hostname.
If the deployment takes place in one of these special environment, the specific hostname of that environment triggers the unique script together (or parallel) with the basic script.
(\cite{humble2010continuous})

\subsubsection{Deploy into a copy of your production environment}
It is a good idea to keep the environments for deployment, testing and staging as similar to the later production environment as possible. If the deployed build
is working on these environments, the development team can be confident that the build will run on the live environment later on. This Method is similar to
a betting system. With every stage that is finalize the confidence for a working web application is growing. In the end the possibility that everything runs like
intended is significantly higher. There is still be the general possibility of failure involved because there is no 100 percent assurance, but the
failure can be minimized to a minimal part. (\cite{humble2010continuous})

\subsubsection{Each change should propagate through the pipeline instantly}
As shown in the description of the different stages, every push to the version control system triggers the commit stage and is following the pipeline, as long as
build and tests are passing. But what happens when other developers push to the repository, while unit testing occurred in the commit stage. It may not be a vital
part for small development teams, but even there different developers can push in short time to the repository. This results
in a build that gets tested and deployed, but is actually behind the real build that gots pushed by the team. A solution is an intelligent scheduling
system of your CD pipeline. Every time a stage is finalized the CD server search for changes in the repository, before going to the next stage. In the
example he CD server starts the commit stage after the repository, that is inside the version controls system, changed. After successful unit
testing, the server checks the repository for other changes that where made during the test. If a new commit has been pushed by another developer,
the CD server will not trigger the next testing stage. Instead the system will start building a new build and starts the unit tests, with the now actual build, again.
If other developers commit code the same rule applies to the end of the new commit stage. In this case the build and deploy process slows down, because the commit stage
took longer because of restarting several times in a row. But compared to having a deployed build that is behind the real code, this longer testing time is worth taking.
There is only so much code that can be shipped simultaneously, so eventually the pipeline will reach the end of all testing stages. If the development team is bigger
different commit times can be scheduled. By theory this will stand against the principles of CD. But there will be longer commit times, because if every commit goes through
the whole pipeline, but is not the last commit, the process is ending up with more time until the next deployed build, than with scheduled commit times .
(\cite{humble2010continuous})

\subsection{Starting a continuous delivery pipeline}
Starting a CD pipeline can be done in several ways. Before describing the actual tools, that can be used to build a suitable workflow, this thesis will describe
the basic starting concept for a CD pipeline.
This section assumes that there is a new project which starts at the iteration zero (this means there is no or only few code and tests, or
only prototype material that will be used partly later on). In this case a CD pipeline can be started at the bare minimum and than can grow with the project
or application. The bare minimum in this case is a commit stage that starts after a commit to the repository, a acceptance stage that starts right after the
commit stage and a deployment stage which deploys the application in a production like environment. To start with, non of the stages should do anything (
because there is no test or code right now). The only thing that should happen is that a push triggers the pipeline cascade so that all stages are running
and culminating in an deployment. In doing so there needs to be a placeholder CD pipeline that runs every time you change the repository. To populate the pipeline
and to test the system itself, a small part of code should be used that can be brought in with the single purpose to test the CD pipeline. As an example there
can be a simple hello world example as a console.log or html tag with a unit test that asserts to true if the console.log or html is represented on the DOM.
This minimalistic code can test the CD system, by committing to the repository. The unit test should assert to true. After running the test on the commit stage the html
will be deployed to run an acceptance test (an acceptance test can only be running if a website is deployed to a staging server!). The acceptance test stage should
verify that the console.log or html is visible on the website. This first steps will provide a practical understanding of how the pipeline works and
if it runs according to what is needed to do, with more complicated things later. In the next section the thesis will show and describe different tools
and CD services that can be used to build an effective and stable continuous delivery chain. (\cite{humble2010continuous})


\section{Development and Deployment with MEAN}
\label{section:Development and Deployment with MEAN}

\subsection{Introduction}
After the basic functions and implementations of the CD pipeline are covered, this section will name and explain different tools
to set up the CD Pipeline. This paper has the goal to cover the implementation of a CD pipeline for todays web applications. There is a wide variety
of tools that can be used. This thesis will not cover all of the known tools available. In fact the described software tools are suggestions to build a
stable pipeline for a CD process. To better understand and learn how to build the pipeline with the mentioned software, this paper will provide the building
of a web application based on the MEAN stack as a practical example. MEAN stands for MongoDB, ExpressJS, AngularJS, and NodeJS. There are different papers and
books about CD with different software systems. So far building a CI pipeline with the MEAN stack is not covered in specific.
The reason for choosing the MEAN stack in the CD example application, is that MEAN uses modern frameworks that are commonly practiced by web developers.
Technology, as always will evolve and eventually single page applications build with the MEAN stack will be out of date. But so far this approach holds many
standards in modern web development.

\subsection{Describing the MEAN stack}
Before starting the tools and practices for building the pipeline, a short description of the MEAN stack will be helpful to understand how the different parts of the
stack are working together and, how they will be set up. After this short introduction, a practical example of how to use the whole MEAN stack will be described.
This Example will be a step by step MEAN stack setup which includes code and references to do so. The following lists provide the different parts of the MEAN stack:

\begin{itemize}
  \item MongoDB
  \item ExpressJS
  \item AngularJS
  \item NodeJS
\end{itemize}

\subsubsection{MongoDB}
MongoDB is a non-relational database which is based on a JSON (JavaScript Object Notation) like document structure. In Detail MongoDB documents are using the BSON structure
which is similar to JSON. A document consists of field-value pairs. As an example the following code shows the document structure of a patient inside the mentioned neolexon application.

\begin{verbatim}
  {
  _id: "56e515ee8ad50f08e1886f13",
  picture: "http://placehold.it/32x32",
  age: 40,
  firstname: "Vroni",
  lastname: "Musterhuber",
  gender: "female",
  about: "Some information"
  }
\end{verbatim}

The code example shows a MongoDB document with different field-value pairs. In this case the fields are id, picture, age, first name, last name, gender and
about. The values are the assigned values per field. For example, the value for the field first name is Vroni. The Mongo database itself is a \textit{Collection}
of \textit{Documents}. The above code shows a \textit{Documents} and many \textit{Documents} together are a \textit{Collection}.

MongoDB uses dot notation to access fields inside a \textit{Documents}. If a developer needs to access the first name field in the above document, he or she
needs to concatenate the \textit{Documents} and the field like this <document>.<field>. Let us assume there are many patients inside our collection and we
want to query for all patients that older than 30. In this case the query would contain the following code:

\begin{verbatim}
  db.patients.find({age: {gt: 30}}).sort({age: 1})
\end{verbatim}

patients are representing the \textit{Collection} (all patients in the database). Find, describes what should be done. In this
case there should be  single out specific patients from our database. The criteria is to find all patients with an age greater than 30. The last part is
a modifier which works like a filter. It allows to sort the findings from this query after the age (from youngest to oldest). There are different
chains for a MongoDB query. They are all explained in detail on the MongoDB website under \url{https://docs.mongodb.org/manual/core/crud-introduction/}.

The advantage of using MongoDB in the example application is that there is no need  for another programming language like \textit{SQL} for relational databases.
For Mongo you can use the programming language you are working with in the application. In this case it is JavaScript. Since the MongoDB document structure is
similar to JSON it allows easily access and use to the data inside the database. \cite{chodorow2013mongodb}

\subsubsection{ExpressJS}
The next part of the MEAN stack is the ExpressJS framework. The ExpressJS website describes it as a fast, un opinionated and minimalistic web framework
for NodeJS \url{http://expressjs.com}. In fact there is no need for ExpressJS in the software stack. All the connections and backend operations can be done
with NodeJS alone. The advantage of using ExpressJS is that it helps to connect all the different parts from the backend. That is why the MEAN stack uses the
framework in the first place. The MEAN application for this thesis will show the advantages an application gets from using ExpressJS later in the practical part
and in code examples. ExpressJS holds the necessary parts of the web application together, so the development team do not need to use and configure all backend
and connecting parts manually \cite{brown2014web}. To do this, ExpressJS requires init and configuration files from the web application. As long as
the needed files are put into the right directory in the web application. ExpressJS will find them and makes use of them. ExpressJS also handles sessions managment
for the login and providing error handling on the backend side.

\subsubsection{AngularJS}
If ExpressJS is the framework for the backend, in this case NodeJS, AngularJS is the javascript front end framework. As said for ExpressJS you do not need
AngularJS to develop the application. But Angular will make it more comfortable to write front end functionality. Angular is an MV* framework, which stands
for \textbf{M}odel - \textbf{V}iew - *. The * actually is a placeholder and means that Angular can used in different ways. Either as MVC - Model View Controller
or as MVVM - Model View View Model. This depends on the choices of the development team and what application will be build. The example application that is
explained for this thesis, is a single page MVC application. It will have a Model which handles the logic and a Controller, which refers to different Views of
the application. A MEAN stack application will handle different models, controllers and view so separate concerns between different functionalities.
To read more about AngularJs follow the link to the main documentation site \url{https://angularjs.org}

\subsubsection{NodeJS}
In \textit{Node up and running} (\cite{hughes2012node}) NodeJS is described as a non blocking, scalable javascript framework. The major advantage is that
if the development team is using javascript in the front end, there is no need to transfer functionalities into php or other backend programming languages.
This makes it easy to use the same language for back- and front end. Besides the scalability of a non blocking architecture and the performance
of the V8 javascript runtime engine, NodeJS possess a large packaging ecosystem (node package manager) which makes it comfortable to work with.

\subsection{Continuous delivery pipeline: tools and practices}
After this brief introduction of the different parts of the MEAN stack, the following list shows the tool that will be used for the MEAN stack and a
continuous delivery pipeline. However this is not a complete list. The following tools will be used in the example project:

\begin{itemize}
  \item Yeoman
  \item StriderCD
  \item Karma
  \item Angular Mocks
  \item Protractor
  \item Mocha
  \item Chai
  \item Browserify
  \item Lodash
  \item Istanbul
\end{itemize}

\section{Describing the CD pipeline}
\label{section:Describing CD Pipeline (WIP)}
To begin the setup process the beginning of this chapter shows the versions and requirements the neolexon project uses for the CD pipeline and MEAN stack.
The configuration is split into server and client side functionalities and will be followed a step by step guide on how to setup the web application
and what problems and difficulties can be occur.

\subsection{Software And Versions}
The neolexon project is using \textbf{Ubuntu 14.04} as the server operation system. The following versions are installed on the server:
\begin{itemize}
  \item nodejs: v5.8.0
  \item npm: v2.15.3
  \item git: v2.8.1
  \item mongoDB: v3.2.3
  \item node-gyp: v3.3.0
  \item StriderCD v1.7.7
\end{itemize}

Strider v1.7.7 has some requirements that are shown in the actual documentations. The setup takes care of the required versions for SriderCD v1.7.7.
If there needs to be more information about setting up versions and software on an Ubuntu server there is the opportunity to follow the installation instructions
in the different software documentations. For using the exact versions listed above, it is useful to use the node version manager (nvm) so changes can be
made to node v4.0 if there are higher or lower version installed on the system.

This Thesis assumes that Strider is set up to deploy on DigitalOcean. DigitalOcean offers a virtual private server (VPS) with moderate pricing and scaling
possibilities. The model which is used for deployment includes 1 Gigabyte of transfer space and 20 Gigabyte disk space. If a higher transfer rate or more disk space
is needed into he future, subscriptions can be changed. There is also the opportunity to choose which datacenter stores the application data. In the neolexon example
the data of patients should stay in the european union for privacy and data security reasons. If this is an option to consider for your application, this can be a useful
feature to choose. As most services today do, you can use your DigitalOcean VPS together with an SSH key to log in.

In order to use the DigitalOcean service with your Ubuntu server an account setup is needed. This can be found under \url{http://digitalocean.com/registrations/new}. After
registration and confirmation via email, the next step is to create a droplet for the Ubuntu 14.04 server (\cite{droplet}). A droplet is the DigitalOcean name for a their VPS.
In the example the project makes use of the Ubuntu 14.04 x64 droplet. The setup in general is a straight forward process. In case of difficulties or problems DigitalOcean has
created a tutorial for this. It can be found on the digitalocean website \footnote{https://www.digitalocean.com/community/tutorials/how-to-create-your-first-digitalocean-droplet-virtual-server}.
The project in use will be using the 64-bit structure for the new VPS. The choosen VPS size uses 512MB of RAM. With RAM of 1 Gigabyte or less the performance benefits of a 64-bit
architecture will be diluted because of the less memory. However, this will be the system of choice because of a future upscale of the application itself. If the plan
to scale the application this can be an option to reconsider.

To use the newly created VPS there is a login via SSH key. The key is send from DigitalOcean, after droplet/VPS is created. The SSH login authentication has a security
advantage over the common login practices. To install the needed software, versions and updates, the \textit{ssh root@<droplet/vps ip>} can be used inside the
terminal/console. Authenticate to the host can be done by setting up a fingerprint for the connection (following the instruction at login). For security reasons
using SSH is recommended. It is also recommended to set a password with the \textit{passwd} command.

\subsection{Installing The MEANJS Application}
The neolexon example project uses the MEANJS yeoman generator to setup the web application. If you are not familiar with the yeoman service the next lines
will summarize what it does and how to use the generator for the application setup. Yeoman is a scaffolding tool that can be installed with npm. What yeoman offer is
installation and scaffolding of various applications. For this different generators are available which generate the wanted application. The MEANJS generator
was made by the MEANJS team and it is documented on their site (\url{http://meanjs.org/generator.html}). Generators install and configure the necessary software.
They also using package managers like bower and npm to install dependencies that are needed. The provided generator was the main reason to use MEANJS over MEAN.io.
The two stacks are similar in use and what they do. However, setting up a MEAN application through yeoman is a fast an straightforward process that helps to bind all
the different parts together. First yeoman needs to be installed. The basic terminal command with npm is

\begin{lstlisting}
  npm install -g yo
\end{lstlisting}

Yeoman will be installed globally and can be used to install the generator. The command for installing the MEANJS generator is

\begin{lstlisting}
  npm install -g generator-meanjs
\end{lstlisting}

The last step in the setup process is to call the installed generator, so that it will build the MEANJS application. The following command will do this

\begin{lstlisting}
  yo meanjs
\end{lstlisting}

Yeoman will take care of installing the application. It will prompt questions about the developers name, company name, origin and description of the application
for the package json file during the process.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/yomean.png}
  \caption{Yeoman MEANJS Generator}
\end{figure}

As soon as the application is installed and build it can be started it with

\begin{lstlisting}
  grunt
\end{lstlisting}

The grunt task runner starts the application which will be available in the browser under \textit{localhost:3000}.
Note that the yeoman generator also created the testing environment for the unit tests. The next step is to deploy this basic MEAN application
to the DigitalOcean VPN by configure striderCD.

\section{Strider Configuration}
\label{section:Strider Configuration}
Installing striderCD needs to clone it first and than install onto the Ubuntu 14.04 server. Strider should use an own user on the server.
If the project in use is an internet accessible project than the own strider user prevents the server from unauthorized access, because
the strider user has only access to the strider service and nothing else. To add the strider user type in the command
\begin{lstlisting}
  addUser strider
\end{lstlisting}
With the strider user created and used, clone and install strider on the server. StriderCD has all the resources that are need on their github page.
To clone them use
\begin{lstlisting}
  git clone https://github.com/Strider-CD/strider
\end{lstlisting}
or visit Github and search for the StriderCD github link.

The installation is done by the npm package manager with
\begin{lstlisting}
  npm install
\end{lstlisting}

During the setup of the neolexon project there where no problems or version conflicts during the setup process. In case of problems and errors
the package manager (npm) will show the error messages. Errors should be fixed immediately before going on with the CI pipline process. After a
successful installation. MEANJS needs a mongoDB database running on your server. The starting command for the strider service is
\begin{lstlisting}
  npm start
\end{lstlisting}
or
\begin{lstlisting}
  bin/strider
\end{lstlisting}
To use a more secure approach the mongoDB should be used with authentication. The command
\begin{lstlisting}
  mongo
\end{lstlisting}
will provide the mongoDB shell to input queries and requests. The authentication plan is to create a mongoDB user which has access
rights to the database that will be create and used for striderCD. The following code snipped shows how to to this
\begin{lstlisting}
  use admin
  db.auth( <username>, <password> )

  db.createUser({user: "strider",
  pwd: "striderPassword",
  roles: [{role: "dbOwner", db: "mean-dev"}]})
\end{lstlisting}
The output should show the created user like this
\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{images/createUser.png}
  \caption{Creating user Strider}
\end{figure}
In case you dont use the standard local mongoDB database you need to specify the port of the mongoDB you are using by
\begin{lstlisting}
  mongo --port <your-port-number>
\end{lstlisting}
For more setup information the mongoDB documentation holds is helpful. The above snippets are
used from the documentation manual 2.6 which can be found on the mongodb website \footnote{https://docs.mongodb.org/manual/reference/method/db.auth/#db.auth}.

To use the proper connecting via mongoose, to the used database, an connection string for mongoose is needed. According to the mongoDB
documentation, this can be done with the following command
\begin{lstlisting}
  mongodb://[username:password@]host1[:port1][,host2[:port2],
  ...[,hostN[:portN]]][/[database][?options]]
\end{lstlisting}
In the case of the created strider user above the connection string reads
\begin{lstlisting}
  DB_URI="mongodb://strider:striderPassword@localhost:27017/strider"
  npm start
\end{lstlisting}

To run and use strider as intended there are several configuration steps that are needed to be done. Strider can either be configured
with system wide environment variables or in a config file. It can configured with variables that are only valid for the current login and
startup. To use process variables for the example project with a port of 4000 and a server name https://neolexon.com the following
command start the process

\begin{lstlisting}
  SERVER_NAME="https://neolexon.com" PORT=4000
  npm start
\end{lstlisting}

If strider needs a restart the process variables are lost and needs to be used again in the startup process. By using a configuration file
the used variables are present for each start of strider. There is no need to retype them in the startup process. It is recommended within
the strider documentation that the configuration file should go inside the strider main directory named

\begin{lstlisting}
  .striderrc
\end{lstlisting}

We recommend implementing an upstart job that makes use of environmental variables to handle start, stop and status evaluation.
The reason for this is that, first, strider uses always the environmental variables, if they are available. Even if there is a configuration
file which makes use of other variables, strider choose to use the environmental variables. The second reason is that in the process of
registering github as a plugin you need to use the github id and a github secret to bind github to strider. plugins needs to be implemented
as environmental variables via export. If the plugin variables for the github plugin are inside a strider configuration file they cannot
be used and therefore the automatic build process after new code is pushed to the repository will not start. This facts results into problems
for the continuous integration pipeline of the neolexon project, because of the implementation of the github variables into the configuration
file of the strider system. After realizing the problem, the step to avoid it was to create environmental variables for this which made it work.
As mentioned above, strider always choose the environmental variables over any configuration file. Since this can result into other errors
it is not advised to split the main configuration into different parts. The solution was to only use environmental variables for the whole service.
The variables are using the following path

\begin{lstlisting}
  /etc/init/strider.conf
\end{lstlisting}

As seen in the following code example the neolexon project only uses environmental variables with strider or for their plugins. The recommend using
for starting is the following inital command in the configuration file:
\begin{lstlisting}
  start on startup
\end{lstlisting}
This command, however did not work in the project. After changing the command to
\begin{lstlisting}
  start on filesystem
\end{lstlisting}
the startup was functioning.
Start on filesystem does the same for the upstart job but it starts if the whole server starts.

\lstinputlisting{files/strider_config.js}

The mentioned and integrated upstart job is a daemon for ubuntu systems, which allows to use striderCD as a service and therefore makes start and
stop handling easier to do. It also provides starting the strider service automatically if the ubuntu server starts or canceling it if the server
stops. This is helpful because it creates automatization that prevents manual starting. For more on upstart visit \url{http://upstart.ubuntu.com/}.

After these steps the installation and standard configuration of strider will be complete. Per default strider uses the development mode to start in.
With the following command the start will changed to the production environment as the new default

\begin{lstlisting}
  NODE_ENV=production
  npm start
\end{lstlisting}

To get an overview about strider and how it works, there are several essential parts, that needs to be understand, to work and configure
strider in the right way. Strider supports different needs and options for testing and deploying web applications. An essential part of
the configuration process is the strider plugin system. Strider uses plugins to integrate services like Github, Slack, Email
notification, Docker containerization and others. To see the complete plugin ecosystem visit \url{https://github.com/Strider-CD/ecosystem-index/blob/master/plugins.yml}.
What can Plugins do for the web application. They generate hooks, register routes for requests, using sockets or editing the strider
UI, depending on what the project needs.

Plugins can be configured and used in two different ways. The first one is with strider UI. Strider UI is strider dashboard.
The strider dashboard hosts several options for configuration. One of them is plugin installation.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/striderUI.png}
  \caption{Strider UI Plugins}
\end{figure}

The second possibility is the installation and configure with the terminal. In this case the command is

\begin{lstlisting}
  bin/strider install <plugin-name>
\end{lstlisting}

In general plugins are used by strider in two existing and different environments. In the web application environment plugins can be manipulated
in the described strider UI. The worker environment uses jobs that strider runs, every time a new commit gets pulled from the git
repository. There are different workers available. The neolexon web application uses the default worker \textit{strider-simple-runner}
(\url{https://github.com/Strider-CD/strider-simple-runner}). A strider job can be code testing or loading some specific scripts or
configuration for plugins. A use case scenario is the deployment to different environments. Workers can load configurations needed for
a specific environment, which is useful in case the application gets deployed to several machines with different setups.

\subsection{Connection To Github}
\label{subsection:Connection To Github}
The goal of the pipeline is to test and deploy the project after a successful commit. StriderCD needs to be connected to the version
control system in use to to this. The example project uses Github as VCS. For this purpose the web application needs to be pushed to a github repository.
Strider UIs plugin page can be used to install the github plugin for this.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/plugin_page.png}
  \caption{Github Plugins}
\end{figure}

In the plugin view the needed plugin can be installed, updated or removed. We need to link the github account and the project repo to striderCD.
The linking between the account takes place in the account settings. From there linking to services is possible. If you choose add account, strider
will guide you through the authorization process.


After authorize the github account the \textit{Github App Id} and the \textit{Github Secret} is needed. Both of them are put
into

\begin{lstlisting}
  /etc/init/strider.conf
\end{lstlisting}

to link the account. The linked account should be showing inside the Strider UI, after the linking was successful.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/linkedGithub.png}
  \caption{Linked Github Account}
\end{figure}

After the accounts are linked, the projects tab shows all github repositories for the added github account. From there the needed projects can
be added to strider.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/neolexonGit.png}
  \caption{Neolexon Repository Overview}
\end{figure}

Note that if there are errors during the installation of plugins, the reason can be that you are not logged in as an administrator,
or that the project trying to link is not a project from the logged in account. StriderCD highlights this in the project tab with a lock.
If another admin or user try to install plugins for a project they do not own, there will be a permission error from git (128)
In the project at hand there where some confusions about why some persons can add plugins and other are not able to.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/permissionError.png}
  \caption{Git 128 Permission Error}
\end{figure}

\subsection{Manual And Automatic Testing With Strider}
\label{Manual And Automatic Testing With Strider}
The next step is to test if strider is connected properly and that tests can be run. The strider UI let you run a manually test from
the dashboard. Strider also indicates that it is running the testing and an approximated time for the test run. After completion
strider shows if the test successful (figure 14). As seen on the picture all operations strider do are logged and can be
analyses. If the test is not running successful it is advised to evaluate the log files and errors and reconfigure according to theses
findings in the project settings.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/manualTest.png}
  \caption{Trigger Manual Test}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/successfulManualTest.png}
  \caption{Manual Test Successful}
\end{figure}

If the manual testing is running correct the next step is to enable strider to run tests automatically after a new push to the
linked repository. For this an update file for the hooked git repository is needed. To create or open these file the path in
the example project is

\begin{lstlisting}
  /home/strider/builds/data/nvpsswaroop-
  neolexon-core-570f544255aebd0a04199f2d/.git/hooks/update
\end{lstlisting}

In the example project case nvpsswaroop-neolexon-core-570f544255aebd0a04199f2d is the project repository from github which is hooked to strider.
It holds the data and build which is created by strider. The name depends on your project name and location on the server.

Since this thesis treats the subject of CI and not testing in general it will not describe testing in detail. Note that for the
most basic functionalities and modules in the MEAN application we use in the example, karma and mocha tests are already in place.
This is a good starting point for learning about testing and how to write and use tests. There are also many resources
on the main pages of mocha (\url{http://mochajs.org}) and karma (\url{https://karma-runner.github.io/0.8/index.html}).Stackoverflow
shows variety of questions and solutions treating unit tests in general and mocha/karma in specific.

\subsection{SSH Deployment}
\label{section:SSH Deployment}
StriderCD also allows deployment with SSH. For this the plugin section holds the SSH Deploy plugin, which is enabled in the same
as other plugins. If enabled, the SSH Deploy plugin is visible in the right side list and can be clicked to modify and configure it. In this
section it is possible to implement the deployment script with the needed commands. It also holds the Private and Public keys that can be used
for authentication with the deployment server.

\subsection{Build Notifications}
\label{section:Build Notifications}
Now that the build process is running (right now there are no tests that are implemented), it is a
good idea to use the full potential of a CI service. Automatization is a useful tool, but the development team needs to be aware of
what is going on. Is the build passing, or are there errors that are interesting for the development team? Should other people
know about the status of the deployment and the build? One of the good things about the different CI services is that there is the
possibility of notification about what is going on. In the following test scenario a developer would like to push some changes to the repository.
He or she would like to know if everything works as intended. If strider gets the changes, it should pull the repository and build the
project. Than strider should store it on the server in the strider build directory for later use. For implement a notification system is useful.
The outcome should be an email or slack message with the build status and if there is anything that needs to be done afterwards. For example
the invitation to deploy the build to the staging or production server.

\subsubsection{Slack Notifications}
The first notification examples uses slack. Slack is a team communication tool. Slack can used to write messages, share files, integrate calendar or
github repositories or use software bots for time management an other useful things. Slack uses multiple channels for communication. For this example there
will be a notification in the build channel that was designed for strider notifications. For more about slack visit the website \url{https://slack.com}.

To start the integration strider needs to interact with the slack application. Like with the github hook strider needs the slack
plugin installed in the Admin/Plugins tab. In this case the slack plugin is already pre installed with strider. After the plugin installation
is complete, the slack plugin needs to be activated and configured to create a web hook that regulates the notifications.
Make sure that the slack plugin is enabled. The plugin can be found under available plugins.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/slack.png}
  \caption{Slack Plugin}
\end{figure}

From there slack can be configure by using the link (figure 11)

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/slack_link.png}
  \caption{Slack Link}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/slack_config.png}
  \caption{Slack Configuration}
\end{figure}

In the configuration window the button \textbf{Fill Empty Fields} can be used to fill in standard messaging and notification options. They can be
configured to the teams liking. For now they stay like this. The only part that is not filled automatically is the web hook URL in the first line.
Setting up the web hook is a straight forward process. By clicking on \textbf{Create an incoming web hook} there is a link to the slack integration
side of the used project (note: A slack team is needed, for which integrations can be added. Visit \url{https://slack.com} for more informations about
this). Creating the web hook produces a web hook URL for the strider slack configuration. The web hook needs to be copy and past into the first line
of the slack configuration. After using and saving the web hook URL the notification system can be tested by using the \textbf{Test} button.
The outcome in your slack application should look similar to the following figure

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/slack_test.png}
  \caption{Slack Notification Test}
\end{figure}

With the next push to your github repository, strider should send a message about the build status directly to the chosen slack channel.
In this case, the build channel shows the message after the next successful push and build.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/slack_buildtest.png}
  \caption{Slack Build Notification Test}
\end{figure}

\subsubsection{Email Notifications}
If there are team members outside the slack team or if you do not using slack notifications, the build process can also be configured
for email. The email notification system can be setup in the same way as done it with slack. First the \textbf{Email Notifier}
plugin needs to be installed.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/email_notifier.png}
  \caption{Email Notifier Plugin}
\end{figure}

Part two is to activate the installed plugin on the \textbf{Plugin Settings Page}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/email_active.png}
  \caption{Email Plugin Enabled}
\end{figure}

The email notification only sends an email if the build status is changing. This means if the build is passing several times and the next
build is failing, an email is send. If this needs to be changed, it can be using the \textbf{Email Notifier} link in the project settings on the left side.
There are no fields for using a specific mail address for the notifications like in the slack settings. To set up a mail address, strider needs an environmental
variable found in our

\begin{lstlisting}
  /etc/int/strider.conf
\end{lstlisting}

In the strider documentation \url{https://github.com/Strider-CD/strider#configuring} it is mentioned that there needs to be a SMTP service in use. This
example makes use of mailgun for sending out the mail notifications. Mailgun offers 300 free emails per day which is sufficient for our project. If needed
there is a subscription later one. After creating an account at \url{https://mailgun.com/signup} and activating the account with mail and/or sms authorization,
the following informations are needed for the strider.conf file. \textbf{HOST}, \textbf{USER} and \textbf{PASS}. In this case the host is mailgun. The user
and pass are your user data from the mailgun account. \textbf{PORT} and \textbf{FROM} are set to a default value which can be kept.
Use the following start command

\begin{lstlisting}
  SMTP_HOST="smtp.mailgun.org
  SMTP_PORT=587 SMTP_USER="your-username" \
  SMTP_PASS="your-password" SMTP_FROM="your-email" \
  ./bin/strider
\end{lstlisting}

To export the variables into the strider.conf file use

\begin{lstlisting}
  export SMTP_HOST="your-smtp-server"
\end{lstlisting}

\cite{strider} und
\cite{stridergithub}

\section{Problems and errors}
\label{section:Problems and errors}

\subsection{Single Point Of Failure}
\label{section:Single Point Of Failure}
Building an automated service like the continuous integration for the neolexon project, is often bind to the use of third party software and services.
The example makes use of digital ocean for deployment, slack and email notification for information about the build status, github for code sharing
via repositories and strider itself as the CI tool. Every application depending on third party services is in danger of the single point of failure
issue. \textbf{SPOF} means that the application or, in this case, the build and deployment process, depends on other software that is integrated in the
project and that there is the ability to fail. If digital ocean is experiencing a downtime during the push of new developed code, the deployment
process will stop in that case and the development team do not have the ability to fix the problem. In this case digital ocean is the only party
who can work on the problem and eventually fix it. Because of this creating dependencies throughout your project should be as minimal as possible.
Concentrate on the necessary software and third parties and try to avoid dependencies that can cause problems without a certain value for the project
or the customer. A working continuous delivery cycle is only as good as the uptime of the different and single components that represent the CI service.

Choose the software that is useful and reliable and that fits the needs of the project. In case of the neolexon CI the decision for striderCD came from
multiple sources of information. Since there is no research literature about strider the neolexon development and dev ops team uses stack overflow and the
github issue report system to evaluate what CI software was fitting and considered stable for the project. That said it is helpful to check the requirements
for every project. Some of the questions regarding the choice of your software are:

\begin{itemize}
  \item What programming language does the system use?
  \item Are there limitations for the deployment the project needs?
  \item What are the alternatives to that software?
  \item Do i need this service or software?
  \item Is the cost value of the service of third party software sufficient?
\end{itemize}

SPOF cannot be fully avoided in todays web development and application management. In \cite{ulbrich2012eliminating} another solution to avoid SPOF is
the use of redundant systems or similar systems and software. Redundancy means that in case of failure another system took over. A useful example
can be another CI service besides strider that took over if strider suffers from downtime. An automatization like this can be useful for bigger
projects that will suffer heavy consequences in case of a failure. For reasons of simplicity and complexity the neolexon project uses no redundant
CI system. Considered the project size and the construction of the neolexon project, managing and creating redundancies would be exaggerated.
\cite{ulbrich2012eliminating}

\subsection{Server Stability And Problems}
\label{section:Server Stability And Problems}
Besides SPOF there can be other problems during the process of setting up the continuous integration system. In the example case the development team
experienced regular server crashes. If this is happening to the project at hand, the solution can be investigation the mongodb swap space.
The swap space provides extra memory for situations in which the database needs it. In the neolexon project the issue could be solved by using the official
mongodb documentation \url{https://docs.mongodb.org/manual/faq/diagnostics/}. The diagnosis section is holding the commands that are needed to find out about the problem with the swap space. In our case
the solution was to reset the database. Because the project was in its early state, this was not a problem. The only data inside the database was testing and
dummy data.

\newpage
\section{Conclusion}
\label{section:Conclusion}
The development process of the continuous integration service of the neolexon web application starts as a straightforward process. The software stack
used for this project is well known and described. during the initial research phase the literature and work of Martin Fowler where extraordinary
helpful. In terms of agile software development and continuous integration, there are several frameworks and services available to choose from to build
state of the art web projects. One of the main challenges where the choice of services and software. The goal of the most frameworks out there is
the same. Let developer teams build and maintain a continuous integration service, that is easy to setup. But what is the right service for the given project?
To solve this challenge the research phase of this thesis included the issue sections of github (if a service was available there). StriderCD is well
documented and uses github for distribution. Knowing the issues of a software service can help in the decision process. The Nodejs focus of strider
was another reason to choose this service. In terms of usability and maintainability, striderCD is easy to use and implement. The configuration is having
some difficulty parts that needs to be understood to configure strider in the right way. Strider CD runs was running in a stable way on the example setup
of Ubuntu 14.04 with deployment on Digital Ocean. The experienced shutdowns where Ubuntu specific and the documentation did well in naming the most
successful versions for the use of strider. During the writing of this thesis and the implementation of the striderCD service the development team did
not experience any downtime of the service itself. According to the build process every push was build after the configuration was complete. The answer
to the research question is that the Issue reports and the experienced downtime together with the concluded build processes are supporting the claim that
a striderCD system can be used as a stable CI service for NodeJS/MEAN applications.

\section{Discussion}
\label{section:Discussion}
It needs to be further tested and maintained that a MEAN or Node application can grow together with the striderCD service and stays stable and reliable
over time. It is recommended to review different systems in different states of the development. Also changes in versioning of third party software
needs to be evaluated. If there are dependencies that are using higher versions of npm as striderCD there needs to be more testing and development
to make sure that the systems are working together. Web development is a fast growing field. The constant change of methods and new approaches (React)
are challenging but helpful. This change needs to be addressed if the systems are changing. The used services that are in use should evaluated or replace
for a future use.

\newpage
\section{List of abbreviations}
\label{section:List of abbreviations}

\begin{itemize}
  \item CD = Continuous Delivery
  \item CI = Continuous Integration
  \item SPOF = Single Point Of Failure
  \item VPS = Virtual Private Server
  \item VCS = Version Control System
  \item MEAN = Framework with MongoDB ExpressJS AngularJS NodeJS
  \item JSON = JavaScript Object Notation
  \item MVC = Model View Controller
  \item SSH = Secure Shell
  \item NPM = Node Package Manager
  \item VPN = Virtual Private Network
  \item URL = Uniform Resource Locator
  \item SMTP = Simple Mail Transfer Protocol
\end{itemize}
